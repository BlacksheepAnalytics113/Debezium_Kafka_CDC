{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4648d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Java\\jdk-11\n"
     ]
    }
   ],
   "source": [
    "# Debezium Lakehouse Integration\n",
    "\n",
    "\"\"\" 1. This notebook demonstrates how to query data captured by Debezium and stored in Apache Iceberg tables.\\\n",
    "    2. Query Customers Table\n",
    "The Spark session is already configured to use the REST Catalog, so we can directly query the tables created by the Iceberg Connect Sink.\n",
    "\"\"\"\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['JAVA_HOME'] = r'C:\\Program Files\\Java\\jdk-11'\n",
    "print(os.environ['JAVA_HOME'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d682a66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import logging\n",
    "config = configparser.ConfigParser()\n",
    "packages = [ \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,\"\n",
    "                     \"org.apache.hadoop:hadoop-common:3.3.6\",\n",
    "                    #  \"org.scala-lang:scala-library:2.13.0\"\n",
    "                    \"com.amazonaws:aws-java-sdk:1.11.469\"\n",
    "]\n",
    "jars = \",\".join(packages)\n",
    "cluster_manager=\"spark://164.92.85.68:7077\"\n",
    "\n",
    "def create_sparksession():\n",
    "    spark = SparkSession.builder.appName('Debezium_lakehouse')\\\n",
    "                .config('spark.jars.packages',jars)\\\n",
    "                .config('spark.hadoop.fs.s3a.aws.credentials.provider','org.apche.hadoop.fs.s3a.impl.SimpleAWSCredentialsProvider')\\\n",
    "                .config('spark.master',cluster_manager)\\\n",
    "                .getOrCreate()\n",
    "   \n",
    "                #  .config('spark.hadoop.fs.s3a.endpoint', 's3://d2b-internal-assessment-dwh.cxeuj0ektqdz.eu-central-1.rds.amazonaws.com')\\\n",
    "                # .config('spark.hadoop.fs.s3a.path.style.access', 'true')\\\n",
    "                # .config('spark.hadoop.fs.s3a.connection.ssl.enabled', 'true')\\\n",
    "                # .config('spark.hadoop.fs.s3a.fast.upload', 'true')\\\n",
    "                # .config('spark.hadoop.fs.s3a.connection.maximum', '1000')\\\n",
    "\n",
    "    spark.sparkContext.setLogLevel('WARN')\n",
    "    logging.info('Spark session created successfully')\n",
    "    return spark   \n",
    "create_sparksession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36f3eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Querying the customers table\n",
    "customers_df = spark.sql(\"SELECT * FROM my_database.customers_table LIMIT 10\")\n",
    "customers_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474f352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Querying the products table\n",
    "products_df = spark.sql(\"SELECT * FROM my_database.products_table LIMIT 10\")\n",
    "products_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bafc4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Querying the orders table\n",
    "orders_df = spark.sql(\"SELECT * FROM my_database.orders_table LIMIT 10\")\n",
    "orders_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b415e17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
